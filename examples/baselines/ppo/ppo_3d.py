# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppo_continuous_actionpy
import os
import random
import time
from dataclasses import dataclass

import gymnasium as gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import tyro
from torch.distributions.normal import Normal
from torch.utils.tensorboard import SummaryWriter

# ManiSkill specific imports
import mani_skill.envs
from mani_skill.utils.wrappers.flatten import FlattenActionSpaceWrapper
from mani_skill.utils.wrappers.record import RecordEpisode
from mani_skill.vector.wrappers.gymnasium import ManiSkillVectorEnv

@dataclass
class Args:
    exp_name: str = os.path.basename(__file__)[: -len(".py")]
    """the name of this experiment"""
    seed: int = 1
    """seed of the experiment"""
    torch_deterministic: bool = True
    """if toggled, `torch.backends.cudnn.deterministic=False`"""
    cuda: bool = True
    """if toggled, cuda will be enabled by default"""
    track: bool = True
    """if toggled, this experiment will be tracked with Weights and Biases"""
    wandb_project_name: str = "cleanRL"
    """the wandb's project name"""
    wandb_entity: str = None
    """the entity (team) of wandb's project"""
    capture_video: bool = True
    """whether to capture videos of the agent performances (check out `videos` folder)"""
    save_model: bool = True
    """whether to save model into the `runs/{run_name}` folder"""
    upload_model: bool = False
    """whether to upload the saved model to huggingface"""
    hf_entity: str = ""
    """the user or org name of the model repository from the Hugging Face Hub"""
    evaluate: bool = False
    """if toggled, only runs evaluation with the given model checkpoint and saves the evaluation trajectories"""
    checkpoint: str = None
    """path to a pretrained checkpoint file to start evaluation/training from"""

    # Algorithm specific arguments
    env_id: str = "PickCube-v1"
    """the id of the environment"""
    total_timesteps: int = 10000000000
    """total timesteps of the experiments"""
    learning_rate: float = 3e-4
    """the learning rate of the optimizer"""
    num_envs: int = 64
    """the number of parallel environments"""
    num_eval_envs: int = 8
    """the number of parallel evaluation environments"""
    partial_reset: bool = True
    """toggle if the environments should perform partial resets"""
    num_steps: int = 50
    """the number of steps to run in each environment per policy rollout"""
    num_eval_steps: int = 50
    """the number of steps to run in each evaluation environment during evaluation"""
    anneal_lr: bool = False
    """Toggle learning rate annealing for policy and value networks"""
    gamma: float = 0.8
    """the discount factor gamma"""
    gae_lambda: float = 0.9
    """the lambda for the general advantage estimation"""
    num_minibatches: int = 8
    """the number of mini-batches"""
    update_epochs: int = 4
    """the K epochs to update the policy"""
    norm_adv: bool = True
    """Toggles advantages normalization"""
    clip_coef: float = 0.2
    """the surrogate clipping coefficient"""
    clip_vloss: bool = False
    """Toggles whether or not to use a clipped loss for the value function, as per the paper."""
    ent_coef: float = 0.0
    """coefficient of the entropy"""
    vf_coef: float = 0.5
    """coefficient of the value function"""
    max_grad_norm: float = 0.5
    """the maximum norm for the gradient clipping"""
    target_kl: float = 0.1
    """the target KL divergence threshold"""
    eval_freq: int = 250
    """evaluation frequency in terms of iterations"""
    finite_horizon_gae: bool = True

    # to be filled in runtime
    batch_size: int = 0
    """the batch size (computed in runtime)"""
    minibatch_size: int = 0
    """the mini-batch size (computed in runtime)"""
    num_iterations: int = 0
    """the number of iterations (computed in runtime)"""

def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
    torch.nn.init.orthogonal_(layer.weight, std)
    torch.nn.init.constant_(layer.bias, bias_const)
    return layer


class PointNet(nn.Module):
    def __init__(self, in_channel=3, output_dim=128):
        super(PointNet, self).__init__()
        self.local_mlp = nn.Sequential(
            nn.Linear(in_channel, 64),
            nn.GELU(),
            nn.Linear(64, output_dim),
        )
        self.reset_parameters_()

    def reset_parameters_(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.trunc_normal_(m.weight, std=.02)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def forward(self, x):
        '''
        x: [B, N, 3]
        '''
        # Local
        x = self.local_mlp(x)
        # global max pooling
        x = torch.max(x, dim=1)[0]
        return x


class Agent(nn.Module):
    def __init__(self, envs_origin_state_dim, output_dim=128):
        super().__init__()
        self.feature_extractor = PointNet(output_dim=output_dim)
        self.state_extractor = nn.Linear(envs_origin_state_dim, output_dim)
        self.critic = nn.Sequential(
            layer_init(nn.Linear(output_dim * 2, 256)),
            nn.Tanh(),
            layer_init(nn.Linear(256, 256)),
            nn.Tanh(),
            layer_init(nn.Linear(256, 256)),
            nn.Tanh(),
            layer_init(nn.Linear(256, 1)),
        )
        self.actor_mean = nn.Sequential(
            layer_init(nn.Linear(output_dim * 2, 256)),
            nn.Tanh(),
            layer_init(nn.Linear(256, 256)),
            nn.Tanh(),
            layer_init(nn.Linear(256, 256)),
            nn.Tanh(),
            layer_init(nn.Linear(256, np.prod(envs.single_action_space.shape)), std=0.01*np.sqrt(2)),
        )
        self.actor_logstd = nn.Parameter(torch.ones(1, np.prod(envs.single_action_space.shape)) * -0.5)

    def get_value(self, pc, state):
        pc_feature = self.feature_extractor(pc)
        state_feature = self.state_extractor(state)
        # pc_feature: (b, 128)
        # state_feature: (b, 128)
        x = torch.concatenate([pc_feature, state_feature], dim=-1)
        return self.critic(x)

    def get_action(self, pc, state, deterministic=False):
        pc_feature = self.feature_extractor(pc)
        state_feature = self.state_extractor(state)
        # pc_feature: (b, 128)
        # state_feature: (b, 128)
        x = torch.concatenate([pc_feature, state_feature], dim=-1)
        action_mean = self.actor_mean(x)
        if deterministic:
            return action_mean
        action_logstd = self.actor_logstd.expand_as(action_mean)
        action_std = torch.exp(action_logstd)
        probs = Normal(action_mean, action_std)
        return probs.sample()

    def get_action_and_value(self, pc, state, action=None):
        pc_feature = self.feature_extractor(pc)
        state_feature = self.state_extractor(state)
        # pc_feature: (b, 128)
        # state_feature: (b, 128)
        x = torch.concatenate([pc_feature, state_feature], dim=-1)
        action_mean = self.actor_mean(x)
        action_logstd = self.actor_logstd.expand_as(action_mean)
        action_std = torch.exp(action_logstd)
        probs = Normal(action_mean, action_std)
        if action is None:
            action = probs.sample()
        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(x)



def get_state_and_pc(obs, env_obs_space, envs_origin_state_dim):
    pc = obs['pointcloud']['xyzw'][..., :3]    # (num_envs, 16384, 3)
    state = None
    env_obs_space = envs.single_observation_space
    if not pc_and_goal_only:
        for key, value in obs['agent'].items():
            if state is None:
                state = value
            else:
                state = torch.concatenate([state, value], dim=1)
        for key, value in obs['extra'].items():
            if state is None:
                state = value
            else:
                state = torch.concatenate([state, value], dim=1)
    else:
        state = obs['extra']['goal_pos']
    return pc, state



pc_and_goal_only = False

if __name__ == "__main__":
    args = tyro.cli(Args)
    args.batch_size = int(args.num_envs * args.num_steps)
    args.minibatch_size = int(args.batch_size // args.num_minibatches)
    args.num_iterations = args.total_timesteps // args.batch_size
    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}_{pc_and_goal_only}"
    writer = None
    if not args.evaluate:
        print("Running training")
        if args.track:
            import wandb

            wandb.init(
                project=args.wandb_project_name,
                entity=args.wandb_entity,
                sync_tensorboard=True,
                config=vars(args),
                name=run_name,
                monitor_gym=True,
                save_code=True,
            )
        writer = SummaryWriter(f"runs/{run_name}")
        writer.add_text(
            "hyperparameters",
            "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
        )
    else:
        print("Running evaluation")

    # TRY NOT TO MODIFY: seeding
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    torch.backends.cudnn.deterministic = args.torch_deterministic

    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")

    # env setup
    env_kwargs = dict(obs_mode="pointcloud", control_mode="pd_joint_delta_pos", render_mode="rgb_array", sim_backend="gpu")
    envs = gym.make(args.env_id, num_envs=args.num_envs, **env_kwargs)

    print("Observation space:", envs.single_observation_space)

    eval_envs = gym.make(args.env_id, num_envs=args.num_eval_envs, **env_kwargs)
    if isinstance(envs.action_space, gym.spaces.Dict):
        envs = FlattenActionSpaceWrapper(envs)
        eval_envs = FlattenActionSpaceWrapper(eval_envs)
    if args.capture_video:
        eval_output_dir = f"runs/{run_name}/videos"
        if args.evaluate:
            eval_output_dir = f"videos"
        print(f"Saving eval videos to {eval_output_dir}")
        eval_envs = RecordEpisode(eval_envs, output_dir=eval_output_dir, save_trajectory=args.evaluate, max_steps_per_video=args.num_eval_steps, video_fps=30)
    envs = ManiSkillVectorEnv(envs, args.num_envs, ignore_terminations=not args.partial_reset, **env_kwargs)
    eval_envs = ManiSkillVectorEnv(eval_envs, args.num_eval_envs, ignore_terminations=not args.partial_reset, **env_kwargs)
    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"

    print(f"GPU memory consumed after env setup: {torch.cuda.memory_allocated() / 1024 / 1024:.2f} MB")

    envs_origin_state_dim = 0
    envs_origin_state_keys = []
    env_obs_space = envs.single_observation_space
    if not pc_and_goal_only:
        for key, value in env_obs_space['agent'].items():
            envs_origin_state_dim += value.shape[0]
            envs_origin_state_keys.append(key)
        for key, value in env_obs_space['extra'].items():
            envs_origin_state_dim += value.shape[0]
            envs_origin_state_keys.append(key)
    else:
        envs_origin_state_dim = env_obs_space['extra']['goal_pos'].shape[0]
        envs_origin_state_keys.append('goal_pos')

    print("total origin state dim = ", envs_origin_state_dim)

    agent = Agent(envs_origin_state_dim).to(device)
    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)

    print(f"GPU memory consumed after agent setup: {torch.cuda.memory_allocated() / 1024 / 1024:.2f} MB")

    envs_original_pointcloud_shape = (16384, 3)

    # ALGO Logic: Storage setup
    obs_state = torch.zeros((args.num_steps, args.num_envs, envs_origin_state_dim)).to(device)
    obs_pc = torch.zeros((args.num_steps, args.num_envs) + envs_original_pointcloud_shape).to(device)

    print(f"GPU memory consumed after storage setup: {torch.cuda.memory_allocated() / 1024 / 1024:.2f} MB")

    actions = torch.zeros((args.num_steps, args.num_envs,) + envs.single_action_space.shape).to(device)
    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)
    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)
    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)
    values = torch.zeros((args.num_steps, args.num_envs)).to(device)

    # TRY NOT TO MODIFY: start the game
    global_step = 0
    start_time = time.time()
    next_obs, _ = envs.reset(seed=args.seed)
    next_pc, next_state = get_state_and_pc(next_obs, env_obs_space, envs_origin_state_dim)
    # next_obs = next_obs['pointcloud']['xyzw'][..., :3]
    eval_obs, _ = eval_envs.reset(seed=args.seed)
    eval_pc, eval_state = get_state_and_pc(eval_obs, env_obs_space, envs_origin_state_dim)
    # eval_obs = eval_obs['pointcloud']['xyzw'][..., :3]
    next_done = torch.zeros(args.num_envs, device=device)
    eps_returns = torch.zeros(args.num_envs, dtype=torch.float, device=device)
    eps_lens = np.zeros(args.num_envs)
    place_rew = torch.zeros(args.num_envs, device=device)
    print(f"####")
    print(f"args.num_iterations={args.num_iterations} args.num_envs={args.num_envs} args.num_eval_envs={args.num_eval_envs}")
    print(f"args.minibatch_size={args.minibatch_size} args.batch_size={args.batch_size} args.update_epochs={args.update_epochs}")
    print(f"####")
    action_space_low, action_space_high = torch.from_numpy(envs.single_action_space.low).to(device), torch.from_numpy(envs.single_action_space.high).to(device)


    def clip_action(action: torch.Tensor):
        return torch.clamp(action.detach(), action_space_low, action_space_high)


    if args.checkpoint:
        agent.load_state_dict(torch.load(args.checkpoint))

    print(f"GPU memory consumed before training loop: {torch.cuda.memory_allocated() / 1024 / 1024:.2f} MB")

    for iteration in range(1, args.num_iterations + 1):
        print(f"Epoch: {iteration}, global_step={global_step}")
        final_values = torch.zeros((args.num_steps, args.num_envs), device=device)
        agent.eval()
        # if iteration % args.eval_freq == 1:
        #     # evaluate
        #
        #     print(f"GPU memory consumed before evaluation: {torch.cuda.memory_allocated() / 1024 / 1024:.2f} MB")
        #     print("Evaluating")
        #     eval_envs.reset()
        #     returns = []
        #     eps_lens = []
        #     successes = []
        #     failures = []
        #     for _ in range(args.num_eval_steps):
        #         with torch.no_grad():
        #             eval_obs, _, eval_terminations, eval_truncations, eval_infos = eval_envs.step(agent.get_action(eval_pc, eval_state, deterministic=True))
        #             eval_pc, eval_state = get_state_and_pc(eval_obs, env_obs_space, envs_origin_state_dim)
        #             if "final_info" in eval_infos:
        #                 mask = eval_infos["_final_info"]
        #                 eps_lens.append(eval_infos["final_info"]["elapsed_steps"][mask].cpu().numpy())
        #                 returns.append(eval_infos["final_info"]["episode"]["r"][mask].cpu().numpy())
        #                 if "success" in eval_infos:
        #                     successes.append(eval_infos["final_info"]["success"][mask].cpu().numpy())
        #                 if "fail" in eval_infos:
        #                     failures.append(eval_infos["final_info"]["fail"][mask].cpu().numpy())
        #     returns = np.concatenate(returns)
        #     eps_lens = np.concatenate(eps_lens)
        #     print(f"Evaluated {args.num_eval_steps * args.num_envs} steps resulting in {len(eps_lens)} episodes")
        #     if len(successes) > 0:
        #         successes = np.concatenate(successes)
        #         if writer is not None: writer.add_scalar("charts/eval_success_rate", successes.mean(), global_step)
        #         print(f"eval_success_rate={successes.mean()}")
        #     if len(failures) > 0:
        #         failures = np.concatenate(failures)
        #         if writer is not None: writer.add_scalar("charts/eval_fail_rate", failures.mean(), global_step)
        #         print(f"eval_fail_rate={failures.mean()}")
        #
        #     print(f"eval_episodic_return={returns.mean()}")
        #     if writer is not None:
        #         writer.add_scalar("charts/eval_episodic_return", returns.mean(), global_step)
        #         writer.add_scalar("charts/eval_episodic_length", eps_lens.mean(), global_step)
        #
        #
        #     print(f"GPU memory consumed after evaluation: {torch.cuda.memory_allocated() / 1024 / 1024:.2f} MB")
        #     if args.evaluate:
        #         break


        # if args.save_model and iteration % args.eval_freq == 1:
        #     model_path = f"runs/{run_name}/{args.exp_name}_{iteration}.cleanrl_model"
        #     torch.save(agent.state_dict(), model_path)
        #     print(f"model saved to {model_path}")
        # # Annealing the rate if instructed to do so.
        # if args.anneal_lr:
        #     frac = 1.0 - (iteration - 1.0) / args.num_iterations
        #     lrnow = frac * args.learning_rate
        #     optimizer.param_groups[0]["lr"] = lrnow

        print(f"GPU memory consumed before rollout: {torch.cuda.memory_allocated() / 1024 / 1024:.2f} MB")

        for step in range(0, args.num_steps):
            global_step += args.num_envs
            obs_pc[step] = next_pc
            obs_state[step] = next_state

            dones[step] = next_done

            # ALGO LOGIC: action logic

            print(f"GPU memory consumed in rollout01: {torch.cuda.memory_allocated() / 1024 / 1024:.2f} MB")
            with torch.no_grad():
                action, logprob, _, value = agent.get_action_and_value(next_pc, next_state)
                values[step] = value.flatten()

            print(f"GPU memory consumed in rollout02: {torch.cuda.memory_allocated() / 1024 / 1024:.2f} MB")
            actions[step] = action
            logprobs[step] = logprob

            # TRY NOT TO MODIFY: execute the game and log data.

            print(f"GPU memory consumed in rollout03: {torch.cuda.memory_allocated() / 1024 / 1024:.2f} MB")
            next_obs, reward, terminations, truncations, infos = envs.step(clip_action(action))
            print(f"GPU memory consumed in rollout031: {torch.cuda.memory_allocated() / 1024 / 1024:.2f} MB")
            next_obs = next_obs['pointcloud']['xyzw'][..., :3]
            print(f"GPU memory consumed in rollout032: {torch.cuda.memory_allocated() / 1024 / 1024:.2f} MB")
            next_done = torch.logical_or(terminations, truncations).to(torch.float32)
            print(f"GPU memory consumed in rollout033: {torch.cuda.memory_allocated() / 1024 / 1024:.2f} MB")
            rewards[step] = reward.view(-1)
            print(f"GPU memory consumed in rollout04: {torch.cuda.memory_allocated() / 1024 / 1024:.2f} MB")


            if "final_info" in infos:
                final_info = infos["final_info"]
                done_mask = final_info["_final_info"]
                episodic_return = final_info['episode']['r'][done_mask].cpu().numpy().mean()

                print(f"GPU memory consumed in rollout041: {torch.cuda.memory_allocated() / 1024 / 1024:.2f} MB")
                if "success" in final_info:
                    writer.add_scalar("charts/success_rate", final_info["success"][done_mask].cpu().numpy().mean(), global_step)
                    print(f"GPU memory consumed in rollout042: {torch.cuda.memory_allocated() / 1024 / 1024:.2f} MB")
                if "fail" in final_info:
                    writer.add_scalar("charts/fail_rate", final_info["fail"][done_mask].cpu().numpy().mean(), global_step)

                    print(f"GPU memory consumed in rollout043: {torch.cuda.memory_allocated() / 1024 / 1024:.2f} MB")
                writer.add_scalar("charts/episodic_return", episodic_return, global_step)

                print(f"GPU memory consumed in rollout044: {torch.cuda.memory_allocated() / 1024 / 1024:.2f} MB")
                writer.add_scalar("charts/episodic_length", final_info["elapsed_steps"][done_mask].cpu().numpy().mean(), global_step)

                print(f"GPU memory consumed in rollout045: {torch.cuda.memory_allocated() / 1024 / 1024:.2f} MB")

            # clean the gpu cache
            torch.cuda.empty_cache()
            # clean the memory
            del action, logprob, value
            del next_obs, reward, terminations, truncations, infos
            #     temp = agent.get_value(final_info["final_observation"][done_mask]).view(-1)
            #
            #     final_values[step, torch.arange(args.num_envs, device=device)[done_mask]] = temp

        print(f"GPU memory consumed after rollout: {torch.cuda.memory_allocated() / 1024 / 1024:.2f} MB")

        # bootstrap value according to termination and truncation
        # with torch.no_grad():
        #     next_value = agent.get_value(next_pc, next_state).reshape(1, -1)
        #     advantages = torch.zeros_like(rewards).to(device)
        #     lastgaelam = 0
        #     for t in reversed(range(args.num_steps)):
        #         if t == args.num_steps - 1:
        #             next_not_done = 1.0 - next_done
        #             nextvalues = next_value
        #         else:
        #             next_not_done = 1.0 - dones[t + 1]
        #             nextvalues = values[t + 1]
        #         real_next_values = next_not_done * nextvalues + final_values[t]  # t instead of t+1
        #         # next_not_done means nextvalues is computed from the correct next_obs
        #         # if next_not_done is 1, final_values is always 0
        #         # if next_not_done is 0, then use final_values, which is computed according to bootstrap_at_done
        #         if args.finite_horizon_gae:
        #             """
        #             See GAE paper equation(16) line 1, we will compute the GAE based on this line only
        #             1             *(  -V(s_t)  + r_t                                                               + gamma * V(s_{t+1})   )
        #             lambda        *(  -V(s_t)  + r_t + gamma * r_{t+1}                                             + gamma^2 * V(s_{t+2}) )
        #             lambda^2      *(  -V(s_t)  + r_t + gamma * r_{t+1} + gamma^2 * r_{t+2}                         + ...                  )
        #             lambda^3      *(  -V(s_t)  + r_t + gamma * r_{t+1} + gamma^2 * r_{t+2} + gamma^3 * r_{t+3}
        #             We then normalize it by the sum of the lambda^i (instead of 1-lambda)
        #             """
        #             if t == args.num_steps - 1: # initialize
        #                 lam_coef_sum = 0.
        #                 reward_term_sum = 0. # the sum of the second term
        #                 value_term_sum = 0. # the sum of the third term
        #             lam_coef_sum = lam_coef_sum * next_not_done
        #             reward_term_sum = reward_term_sum * next_not_done
        #             value_term_sum = value_term_sum * next_not_done
        #
        #             lam_coef_sum = 1 + args.gae_lambda * lam_coef_sum
        #             reward_term_sum = args.gae_lambda * args.gamma * reward_term_sum + lam_coef_sum * rewards[t]
        #             value_term_sum = args.gae_lambda * args.gamma * value_term_sum + args.gamma * real_next_values
        #
        #             advantages[t] = (reward_term_sum + value_term_sum) / lam_coef_sum - values[t]
        #         else:
        #             delta = rewards[t] + args.gamma * real_next_values - values[t]
        #             advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * next_not_done * lastgaelam # Here actually we should use next_not_terminated, but we don't have lastgamlam if terminated
        #     returns = advantages + values


        # print(f"GPU memory consumed after bootstrap: {torch.cuda.memory_allocated() / 1024 / 1024:.2f} MB")
        # # flatten the batch
        # b_obs_state = obs_state.reshape((-1, envs_origin_state_dim))
        # b_obs_pc = obs_pc.reshape((-1,) + envs_original_pointcloud_shape)
        #
        #
        # b_logprobs = logprobs.reshape(-1)
        # b_actions = actions.reshape((-1,) + envs.single_action_space.shape)
        # b_advantages = advantages.reshape(-1)
        # b_returns = returns.reshape(-1)
        # b_values = values.reshape(-1)
        #
        # # Optimizing the policy and value network
        # agent.train()
        # b_inds = np.arange(args.batch_size)
        # clipfracs = []
        # for epoch in range(args.update_epochs):
        #     np.random.shuffle(b_inds)
        #     for start in range(0, args.batch_size, args.minibatch_size):
        #         end = start + args.minibatch_size
        #         mb_inds = b_inds[start:end]
        #
        #         _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs_pc[mb_inds], b_obs_state[mb_inds], b_actions[mb_inds])
        #         logratio = newlogprob - b_logprobs[mb_inds]
        #         ratio = logratio.exp()
        #
        #         with torch.no_grad():
        #             # calculate approx_kl http://joschu.net/blog/kl-approx.html
        #             old_approx_kl = (-logratio).mean()
        #             approx_kl = ((ratio - 1) - logratio).mean()
        #             clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]
        #
        #         mb_advantages = b_advantages[mb_inds]
        #         if args.norm_adv:
        #             mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)
        #
        #         # Policy loss
        #         pg_loss1 = -mb_advantages * ratio
        #         pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)
        #         pg_loss = torch.max(pg_loss1, pg_loss2).mean()
        #
        #         # Value loss
        #         newvalue = newvalue.view(-1)
        #         if args.clip_vloss:
        #             v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
        #             v_clipped = b_values[mb_inds] + torch.clamp(
        #                 newvalue - b_values[mb_inds],
        #                 -args.clip_coef,
        #                 args.clip_coef,
        #             )
        #             v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2
        #             v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)
        #             v_loss = 0.5 * v_loss_max.mean()
        #         else:
        #             v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()
        #
        #         entropy_loss = entropy.mean()
        #         loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef
        #
        #         optimizer.zero_grad()
        #         loss.backward()
        #         nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)
        #         optimizer.step()
        #
        #     if args.target_kl is not None and approx_kl > args.target_kl:
        #         break
        #
        # print(f"GPU memory consumed after optimization: {torch.cuda.memory_allocated() / 1024 / 1024:.2f} MB")
        # y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()
        # var_y = np.var(y_true)
        # explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y
        #
        # # TRY NOT TO MODIFY: record rewards for plotting purposes
        # writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
        # writer.add_scalar("losses/value_loss", v_loss.item(), global_step)
        # writer.add_scalar("losses/policy_loss", pg_loss.item(), global_step)
        # writer.add_scalar("losses/entropy", entropy_loss.item(), global_step)
        # writer.add_scalar("losses/old_approx_kl", old_approx_kl.item(), global_step)
        # writer.add_scalar("losses/approx_kl", approx_kl.item(), global_step)
        # writer.add_scalar("losses/clipfrac", np.mean(clipfracs), global_step)
        # writer.add_scalar("losses/explained_variance", explained_var, global_step)
        # print("SPS:", int(global_step / (time.time() - start_time)))
        # writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)

    if not args.evaluate:
        if args.save_model:
            model_path = f"runs/{run_name}/{args.exp_name}_final.cleanrl_model"
            torch.save(agent.state_dict(), model_path)
            print(f"model saved to {model_path}")
        writer.close()
    envs.close()
    eval_envs.close()
